# 4 questions to help accurately scope analytics engineering projects

Created: Apr 22, 2020 12:18 PM
Tags: Consulting, Data engineering, Productivity, SQL
URL: https://blog.getdbt.com/4-questions-to-help-you-more-accurately-scope-analytics-engineering-projects/

![4%20questions%20to%20help%20accurately%20scope%20analytics%20eng%20c5db38d10eae45038d54f10ddfa7ccbc/scope-1.jpg](4%20questions%20to%20help%20accurately%20scope%20analytics%20eng%20c5db38d10eae45038d54f10ddfa7ccbc/scope-1.jpg)

At Fishtown Analytics, we sell and deliver analytics engineering projects in two-week sprints, each of which has a fixed price and fixed deliverables. Delivering on such a tight timeline with defined outcomes is intense! And weâ€™ve gotten quite good at it: weâ€™ve now delivered over 1,000 sprints, with over 95% of them delivered on time.

As a result, weâ€™ve gotten really, really good at scoping analytics projects. Weâ€™ve found that if we answer four questions up-front, we can significantly reduce the chances of uncovering issues before itâ€™s too late. In fact, **ferreting out issues early is the single most important factor in delivering analytics work on time**. If you spend a week going down one path only to realize itâ€™s a dead end, thatâ€™s a tremendous amount of wasted time. Itâ€™s critical to anticipate the cul-de-sac before you find yourself stuck there.

Our approach to proactive issue identification has become intuitiveâ€”itâ€™s something that we all *just do*, because weâ€™ve all burned our hands on the stove enough times to know better: â€œI shouldâ€™ve known I needed to check whether those IDs would map. [Shit](https://www.youtube.com/watch?v=Fr0A7TofowE).â€

Itâ€™s such an important topic, though, that I wanted to take the time to make our intuitive, tacit knowledge more explicit and share it with the larger community. Because this skill isnâ€™t just for consultants: anyone who is using [Agile on their data team](https://www.locallyoptimistic.com/post/agile-analytics-p1/), or really anyone who manages deadlines on data projects at all, should care.

Why? **Data teams are successful when they build reserves of trust**â€”thatâ€™s how you earn the â€œtrusted advisorâ€ status from all of your business stakeholders, and how you ultimately impact the trajectory of the business. We earn trust by delivering data that is high-quality, tested, and accurate. We also build trust when we set clear expectations about how a project will go and then deliver on them, time after time after time.

Here are the four questions we ask to proactively identify issues and deliver analytics engineering projects on time at Fishtown Analytics.

## Do I have the data from the primary source?

Itâ€™s not unusual to have data that is relevant to your problem but is from the wrong system. One common example: your company might have marketing data stored in Marketo, but it is ingested into your data warehouse through an existing Salesforce integration. Using the Marketo data vis-a-vis Salesforce isnâ€™t *wrong*, but itâ€™s *risky*. The two-hop integration will have more opportunities for failure, and the data will likely not be as granular or complete as the primary source data. Better to get all data directly from its originating source.

There are actually tons of examples of this. Should you trust:

- **Shopifyâ€™s charges or Stripeâ€™s?** I would choose Stripeâ€™s for financial / revenue use cases and Shopifyâ€™s for ecommerce use cases.
- **Mixpanelâ€™s events or Segmentâ€™s?** I would almost always choose Segment, given that most companies push data through Segment to Mixpanel.
- **Hubspotâ€™s attribution or Snowplowâ€™s?** I believe your web analytics tool (Snowplow in this case) needs to be the authoritative source of your web event data, and thus your attribution.

Itâ€™s a funny thing to insist on having data directly from the source when you start a project, because in some ways itâ€™s not very pragmatic. Paying for an extra integration typically has a clear price tag, and itâ€™s not at all clear what the value is at the outset. Why isnâ€™t the data you have â€œgood enoughâ€? The answer is that building your analytics on top of the most fundamentally true source of data saves a tremendous amount of wasted time, frustration, workarounds, and spaghetti code in the future.

**If you have true primary source data, youâ€™re virtually omnipotent**: if a question can be answered, you can answer it. If youâ€™re in a meeting with a VP who says â€œcan we slice leads by acquisition source?â€ the answer will either be â€œyesâ€ or â€œthe systems we use donâ€™t give us the data to do thatâ€. If you donâ€™t have primary source data, you sometimes end up having to answer â€œnot given our current analytics pipelineâ€. Not exactly trust-inspiring.

If you donâ€™t have primary source data, do two things:

1. See if you can ingest that data straightforwardly. This could be as easy as setting up a Stitch or Fivetran integration, or it could require a custom integration (if so, let us know, [we might be able to help out](https://www.fishtownanalytics.com/)!).
2. If you canâ€™t get primary source data and need to rely on a secondary source, alert your stakeholders of the potential drawbacks so that everyone goes into the project with the right expectations. Outline the boundaries of the analysis youâ€™ll be able to provide, and the amount of re-work required down the road if you are able to get at the primary source data. Thereâ€™s nothing inherently wrong with making decisions based on expediency as long as youâ€™ve clearly educated stakeholders on the potential downstream impacts.

## 2. Is source data sufficiently granular?

According to [Kimball](https://www.kimballgroup.com/2007/07/keep-to-the-grain-in-dimensional-modeling/), itâ€™s important to â€œstick to rich, expressive, atomic-level data thatâ€™s closely connected to the original source and collection process.â€ I couldnâ€™t agree more: granular data gives you *options*, aggregated data is limiting.

One of the most common examples of this in practice is the difference between Google Analytics data exported via the API and Google Analytics 360 data loaded automatically into Bigquery. Via the API, GA will only give you aggregated metrics, whereas GA 360 will export every single session and every single event into Bigquery. The aggregated data is only useful when creating extremely high-level analyses: trendlines of page views or unique visitors. With the granular data, you can answer any question you wantâ€”multi-touch attribution, funnel analysis, A/B testing, etc.

Ideally, you want to have access to the absolute most granular data available from the source data system. At this early stage, you donâ€™t yet know all of the followup questions youâ€™re going to be asked to investigate, and you want to be able to follow the trail wherever it leads. Even if your top-level question could be answered by an aggregated export, your followup questions almost definitely wonâ€™t.

Itâ€™s important to actually learn about the source data systems whose data youâ€™re interacting with.

- **If itâ€™s a SaaS product**, read the API docs. What data will the API give you? Good APIs give you all of the objects you need; *great* APIs will give you change tracking on top of those objects. For example, the Salesforce API will give you a history table containing all opportunities and their state changes since the beginning of your account. It turns out, that opportunity history table is critical for a big chunk of Salesforce pipeline reporting.
- **If itâ€™s an internal product database**, read whatever internal documentation is available or sit down with an experienced engineer to understand what data is stored in the product.

Overall, the closer you can get to events, the better off youâ€™ll be. If you have all state changes for a given system, you can always reconstruct the state at any given point in timeâ€”a very powerful position to be in. For example, the only endpoint you actually need from the Stripe API is the events endpoint: from that single endpoint you can derive the outputs from all other endpoints at any point in time. You can do a similar thing for email marketing performance using Mailchimpâ€™s events table.

Fortunately, most Stitch and Fivetran integrations pull maximally granular data by defaultâ€”both of these products were built for the modern analytics stack and want to give you complete control. Itâ€™s important to check for yourself, though. A little while ago I was speccing out a sprint with a client to do sales pipeline reporting on top of the Stitch Close.io integration, and it turns out that integration (which is community-supported) didnâ€™t include the opportunity history endpoint. This dramatically changed the scope of the project as we couldnâ€™t actually produce the analysis required without adding that additional endpoint to the integration. Better that we discovered that at the outset.

## 3. Has this data set been used before?

If your organization *has not* previously used the data from a given ingestion pipeline, you should treat it as guilty until proven innocent. There are a large number of ways in which data that you havenâ€™t used before can be wrong, and many of them are subtle. There are two main categories:

- **Broken ingestion.** There can be errors in the ingestion that cause data in your warehouse to be incorrect. Even when youâ€™re dealing with Stitch and Fivetran integrations, *shit happens*. Every time Iâ€™ve seen an integration break it is for the strangest, most esoteric problems, each is one-of-a-kind.
- **Unanticipated ingestion behavior.** Sometimes, loading happens in a way that you wouldnâ€™t have anticipated. For example, Stitch loads many tables that it calls â€œreporting tablesâ€ as an immutable log and then gives specific instructions on how to query from these tables. If you donâ€™t read their instructions and simply start writing queries, youâ€™ll come up with numbers that are head-scratchingly off.

Both of these types of errors can be caught by *auditing the data* from new data sets before building mission-critical analyses on top of them. What exactly does auditing a data set mean? For us, it means producing straightforward metrics on top of the table/tables that can be compared to the outputs from another system (typically the system that the data was extracted from). For instance, we frequently calculate orders and revenue and compare those metrics with the results we get natively in Shopify. Both of these metrics are very straightforward to calculate and if the numbers match you immediately have a high degree of confidence that the data set is in good shape.

Problems with your raw data are typically cross-cutting: theyâ€™ll affect all records in a table, or all records in a certain date range. If you audit 2-3 metrics, you can generally feel pretty good about the data quality overall.

If your organization *has* previously used the data from a given ingestion pipeline, itâ€™s incumbent that you invest in learning whatâ€™s already been built. If youâ€™re using dbt:

- look up the existing modeling work thatâ€™s been done using dbt docs
- read all of the descriptive text for models and columns, and make sure you actually grok the code
- find the authors in the git commit history, and ask them if there are any gotchas you should know about

**ğŸ›‘Do not just say â€œscrew itâ€ and start your own work from scratch.** The primary source of analytics engineering tech debt and errors result from multiple code bases attempting to transform and analyze fundamentally the same data. Two code paths equates to double the surface area for errors and infinitely more opportunity for confusion. Itâ€™s not always easy to follow the work thatâ€™s been done before you, but *itâ€™s critically important that you integrate into an existing code base rather than starting from scratch.* Iâ€™ve seen teams who are unwilling to write code collaboratively in this way and instead every analyst builds their own silo. **This is a recipe for an incredibly unproductive data team. ğŸ›‘**

## 4. Do the necessary keys exist to join all of the data together?

New analysis typically builds on existing concepts by attaching new data to data that has already been modeled. Itâ€™s typical to start with your application database or shopping cart, and then spider outwards into other data systems: event tracking, customer success, advertising, email, etc. As you bring each one of these systems into your pipeline and your data model, youâ€™ll need at least one key to join the data in.

It seems like that would be fairly straightforward, but often it just isnâ€™t. Here are some examples:

- Youâ€™re registering your trial signups as leads in your Salesforce instance, but the web form on the marketing site that submits to Salesforce doesnâ€™t have the userâ€™s account number at that point in the flow. No key!
- Your event tracking only shows 15,000 identified users over the past week but your application database is confident that there have been 20,000 users logging in over the same time period. The user identification code for your event tracking pipeline is â€œleakyâ€â€”some identify calls are missing! Which ones?
- You want to track customer acquisition cost (CAC) across your entire funnel, starting with ad spend. But when you go to integrate your data from your various ad sources, you realize that the marketing team hasnâ€™t been adding URL parameters to the links theyâ€™ve been using! Without these parameters, you have no way of joining data from the rest of your warehouse to the advertising cost data.

This stuff happens *constantly*, and knowing where to look for problems before you hit them is absolutely critical. If youâ€™re missing a key between two systems, it can stop an entire analytics project dead in its tracks, because often a) adding the key involves bringing in stakeholders from other parts of the org, and b) there is no way to retroactively get the data.

Make sure to check your keys before diving in. If theyâ€™re missing, go through the effort of working with the stakeholders to create them. This â€œprocess instrumentationâ€ is a critical part of what makes a great analytics engineer: you canâ€™t always expect that business processes will naturally spit out the data you need and sometimes youâ€™ll need to roll up your sleeves and make sure it gets gathered.

## Know the answers before you dive in

As a consultant, I have to have a strong process: if I mis-scope a sprint, it probably means that Iâ€™m not going to be getting enough sleep over the coming two weeks. Or maybe it means that we lose $$ on the projectâ€”the stakes are real. Thatâ€™s why I (and everyone at Fishtown) is so focused on being good at scoping.

On an internal data team, the stakes are just as high, but the feedback typically isnâ€™t as clear or immediate. Your stakeholders will notice if you consistently miss deadlines or fail to deliver key results, *they just might not say anything to you*. Itâ€™s often hard to give direct feedback to colleague â€”whenâ€™s the last time you said something like â€œYour teamâ€™s miss on that deadline led me to miss my committed OKR for the quarter and Iâ€™m pissed.â€?

Even if it isnâ€™t made explicit, consistent failure by a data team to deliver predictable outcomes damages trust and thereby damages the teamâ€™s ability to make an impact on the larger org. Avoid this outcome by identifying issues during the scoping phase of a project and by communicating limitations of your approach with stakeholders.